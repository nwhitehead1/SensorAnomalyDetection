{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Network Latency & Anomaly Detection with Autoencoders\n",
    "\n",
    "The goal of this project is twofold:\n",
    "- Reduce the network traffic in the cloud from the gateway layer.\n",
    "- Detect anomalous data, indicating a faulty sensor or a potential attack.\n",
    "\n",
    "We use a subset of data collected from Intel Labs between March and April, 2004 (http://db.csail.mit.edu/labdata/labdata.html) as a proof of concept for applying Deep Learning at the IoT Gateway Layer.\n",
    "\n",
    "In the best case scenario, we can send predicted batches of time series data that are representative of the actual readings in $n/k$ transmissions, where $n$ is the size of our time series set, and $k$ is the batching size.\n",
    "\n",
    "In the worst case scenario, incorrectly predicted batch values will update what is currently in the cloud at the time of sensor reading. This scenario will perform as well as trivially passing data from the gateway to the cloud unhindered in $n$ transmissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('data.txt.gz', 'rb') as data_bytes:\n",
    "    data = pd.read_csv(data_bytes, header=None, sep=' ', parse_dates=[[0, 1]], squeeze=True)\n",
    "data.columns = ['DATETIME','EPOCH','SENSOR_ID','TEMPERATURE','HUMIDITY','LIGHT','VOLTAGE']\n",
    "data = data.set_index('DATETIME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2313682, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider sensor data between March 1st and March 10th for this experiment, as it contains the majority of the complete data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(892574, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_samp = data.loc['2004-03-01':'2004-03-10'].copy()\n",
    "data_samp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of a proof of concept, we will make this a univariate problem (not including DateTime), focusing on Temperature readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samp.drop(['HUMIDITY','LIGHT','VOLTAGE','EPOCH'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping any Sensor ID's where the value is NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samp.dropna(subset=['SENSOR_ID'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of out experiment, let us only consider sensors 1-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samp = data_samp[(data_samp.SENSOR_ID >= 1) & (data_samp.SENSOR_ID <= 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping the Sensor ID field to an integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_samp.SENSOR_ID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SENSOR_ID        int64\n",
       "TEMPERATURE    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_samp.SENSOR_ID = data_samp.SENSOR_ID.astype(int)\n",
    "data_samp.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SENSOR_ID</th>\n",
       "      <th>TEMPERATURE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATETIME</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2004-03-01 00:01:57.130850</td>\n",
       "      <td>1</td>\n",
       "      <td>18.4498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2004-03-01 00:02:50.458234</td>\n",
       "      <td>1</td>\n",
       "      <td>18.4400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2004-03-01 00:04:26.606602</td>\n",
       "      <td>1</td>\n",
       "      <td>18.4400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2004-03-01 00:05:28.379208</td>\n",
       "      <td>1</td>\n",
       "      <td>18.4498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2004-03-01 00:05:50.456126</td>\n",
       "      <td>1</td>\n",
       "      <td>18.4302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            SENSOR_ID  TEMPERATURE\n",
       "DATETIME                                          \n",
       "2004-03-01 00:01:57.130850          1      18.4498\n",
       "2004-03-01 00:02:50.458234          1      18.4400\n",
       "2004-03-01 00:04:26.606602          1      18.4400\n",
       "2004-03-01 00:05:28.379208          1      18.4498\n",
       "2004-03-01 00:05:50.456126          1      18.4302"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_samp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to measure the temperature at each sensor for a given timestamp, so we will pivot the table, making the column values sensor temperature readings at a given timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samp = data_samp.pivot(columns='SENSOR_ID', values='TEMPERATURE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>SENSOR_ID</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATETIME</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2004-03-01 00:00:21.445722</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.489</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2004-03-01 00:00:22.429139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.8712</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2004-03-01 00:00:25.633782</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.7144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2004-03-01 00:00:52.381230</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.8614</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2004-03-01 00:00:53.317719</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.7046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "SENSOR_ID                   1        2   3   4   5   6        7   8       9   \\\n",
       "DATETIME                                                                       \n",
       "2004-03-01 00:00:21.445722 NaN      NaN NaN NaN NaN NaN      NaN NaN  18.489   \n",
       "2004-03-01 00:00:22.429139 NaN  18.8712 NaN NaN NaN NaN      NaN NaN     NaN   \n",
       "2004-03-01 00:00:25.633782 NaN      NaN NaN NaN NaN NaN  18.7144 NaN     NaN   \n",
       "2004-03-01 00:00:52.381230 NaN  18.8614 NaN NaN NaN NaN      NaN NaN     NaN   \n",
       "2004-03-01 00:00:53.317719 NaN      NaN NaN NaN NaN NaN  18.7046 NaN     NaN   \n",
       "\n",
       "SENSOR_ID                   10  \n",
       "DATETIME                        \n",
       "2004-03-01 00:00:21.445722 NaN  \n",
       "2004-03-01 00:00:22.429139 NaN  \n",
       "2004-03-01 00:00:25.633782 NaN  \n",
       "2004-03-01 00:00:52.381230 NaN  \n",
       "2004-03-01 00:00:53.317719 NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_samp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be a lot of missing values for temperature readings in our table, due to micro-second DateTime ID's in our time series set. We will resample the data every 2 minutes, taking the mean of the values collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samp = data_samp.resample('2min').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New resampled set has: 6754 data points.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SENSOR_ID\n",
       "1      143\n",
       "2      629\n",
       "3      114\n",
       "4      349\n",
       "5     6754\n",
       "6      582\n",
       "7       17\n",
       "8      753\n",
       "9       65\n",
       "10     163\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('New resampled set has: {} data points.'.format(len(data_samp)))\n",
    "data_samp.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly sensor 5 is not reading values between our time frame, so we will drop it. Stack brings the prescribed column (SENSOR_ID) into our index, making it easily dropped. We unstack to bring Sensor ID out of the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = data_samp.stack().drop(5, level='SENSOR_ID')\n",
    "data_samp = temp_df.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>SENSOR_ID</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATETIME</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2004-03-01 00:00:00</td>\n",
       "      <td>18.449800</td>\n",
       "      <td>18.864667</td>\n",
       "      <td>18.753600</td>\n",
       "      <td>19.11130</td>\n",
       "      <td>18.6752</td>\n",
       "      <td>18.70705</td>\n",
       "      <td>18.386100</td>\n",
       "      <td>18.484100</td>\n",
       "      <td>18.430200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2004-03-01 00:02:00</td>\n",
       "      <td>18.440000</td>\n",
       "      <td>18.848333</td>\n",
       "      <td>18.756867</td>\n",
       "      <td>19.10640</td>\n",
       "      <td>18.6654</td>\n",
       "      <td>18.69235</td>\n",
       "      <td>18.378750</td>\n",
       "      <td>18.469400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2004-03-01 00:04:00</td>\n",
       "      <td>18.440000</td>\n",
       "      <td>18.832000</td>\n",
       "      <td>18.734000</td>\n",
       "      <td>19.10640</td>\n",
       "      <td>18.6654</td>\n",
       "      <td>18.68500</td>\n",
       "      <td>18.376300</td>\n",
       "      <td>18.475933</td>\n",
       "      <td>18.400800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2004-03-01 00:06:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.851600</td>\n",
       "      <td>18.753600</td>\n",
       "      <td>19.10640</td>\n",
       "      <td>18.6654</td>\n",
       "      <td>18.67765</td>\n",
       "      <td>18.377933</td>\n",
       "      <td>18.482467</td>\n",
       "      <td>18.410600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2004-03-01 00:08:00</td>\n",
       "      <td>18.435100</td>\n",
       "      <td>18.861400</td>\n",
       "      <td>18.773200</td>\n",
       "      <td>19.10150</td>\n",
       "      <td>18.6556</td>\n",
       "      <td>18.68500</td>\n",
       "      <td>18.371400</td>\n",
       "      <td>18.479200</td>\n",
       "      <td>18.433467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2004-03-10 08:58:00</td>\n",
       "      <td>22.835300</td>\n",
       "      <td>23.178300</td>\n",
       "      <td>23.776100</td>\n",
       "      <td>23.93045</td>\n",
       "      <td>24.0946</td>\n",
       "      <td>23.99170</td>\n",
       "      <td>25.296733</td>\n",
       "      <td>26.044800</td>\n",
       "      <td>24.473533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2004-03-10 09:00:00</td>\n",
       "      <td>22.879400</td>\n",
       "      <td>23.134200</td>\n",
       "      <td>23.717300</td>\n",
       "      <td>23.92800</td>\n",
       "      <td>24.1044</td>\n",
       "      <td>23.92800</td>\n",
       "      <td>25.395550</td>\n",
       "      <td>26.113400</td>\n",
       "      <td>24.589500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2004-03-10 09:02:00</td>\n",
       "      <td>22.869600</td>\n",
       "      <td>23.121950</td>\n",
       "      <td>23.676467</td>\n",
       "      <td>23.90840</td>\n",
       "      <td>24.1485</td>\n",
       "      <td>23.95740</td>\n",
       "      <td>25.496000</td>\n",
       "      <td>26.280000</td>\n",
       "      <td>24.692400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2004-03-10 09:04:00</td>\n",
       "      <td>22.836933</td>\n",
       "      <td>23.161150</td>\n",
       "      <td>23.673200</td>\n",
       "      <td>23.89370</td>\n",
       "      <td>24.1436</td>\n",
       "      <td>23.94760</td>\n",
       "      <td>25.518050</td>\n",
       "      <td>26.270200</td>\n",
       "      <td>24.709550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2004-03-10 09:06:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.163600</td>\n",
       "      <td>23.653600</td>\n",
       "      <td>23.86920</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.92800</td>\n",
       "      <td>25.505800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6754 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "SENSOR_ID                   1          2          3         4        6   \\\n",
       "DATETIME                                                                  \n",
       "2004-03-01 00:00:00  18.449800  18.864667  18.753600  19.11130  18.6752   \n",
       "2004-03-01 00:02:00  18.440000  18.848333  18.756867  19.10640  18.6654   \n",
       "2004-03-01 00:04:00  18.440000  18.832000  18.734000  19.10640  18.6654   \n",
       "2004-03-01 00:06:00        NaN  18.851600  18.753600  19.10640  18.6654   \n",
       "2004-03-01 00:08:00  18.435100  18.861400  18.773200  19.10150  18.6556   \n",
       "...                        ...        ...        ...       ...      ...   \n",
       "2004-03-10 08:58:00  22.835300  23.178300  23.776100  23.93045  24.0946   \n",
       "2004-03-10 09:00:00  22.879400  23.134200  23.717300  23.92800  24.1044   \n",
       "2004-03-10 09:02:00  22.869600  23.121950  23.676467  23.90840  24.1485   \n",
       "2004-03-10 09:04:00  22.836933  23.161150  23.673200  23.89370  24.1436   \n",
       "2004-03-10 09:06:00        NaN  23.163600  23.653600  23.86920      NaN   \n",
       "\n",
       "SENSOR_ID                  7          8          9          10  \n",
       "DATETIME                                                        \n",
       "2004-03-01 00:00:00  18.70705  18.386100  18.484100  18.430200  \n",
       "2004-03-01 00:02:00  18.69235  18.378750  18.469400        NaN  \n",
       "2004-03-01 00:04:00  18.68500  18.376300  18.475933  18.400800  \n",
       "2004-03-01 00:06:00  18.67765  18.377933  18.482467  18.410600  \n",
       "2004-03-01 00:08:00  18.68500  18.371400  18.479200  18.433467  \n",
       "...                       ...        ...        ...        ...  \n",
       "2004-03-10 08:58:00  23.99170  25.296733  26.044800  24.473533  \n",
       "2004-03-10 09:00:00  23.92800  25.395550  26.113400  24.589500  \n",
       "2004-03-10 09:02:00  23.95740  25.496000  26.280000  24.692400  \n",
       "2004-03-10 09:04:00  23.94760  25.518050  26.270200  24.709550  \n",
       "2004-03-10 09:06:00  23.92800  25.505800        NaN        NaN  \n",
       "\n",
       "[6754 rows x 9 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_samp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still some missing values, which we can simply deal with by applying linear interpolation to estimate values making our set continuous. Interpolation uses previous values, so for values appearing at the front of our frame (ie. sensor 1) we must make the process bidirectional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samp = data_samp.interpolate(method='linear', limit_direction='both', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SENSOR_ID\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "6     0\n",
       "7     0\n",
       "8     0\n",
       "9     0\n",
       "10    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_samp.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>SENSOR_ID</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>6754.000000</td>\n",
       "      <td>6754.000000</td>\n",
       "      <td>6754.000000</td>\n",
       "      <td>6754.000000</td>\n",
       "      <td>6754.000000</td>\n",
       "      <td>6754.000000</td>\n",
       "      <td>6754.000000</td>\n",
       "      <td>6754.000000</td>\n",
       "      <td>6754.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>22.192462</td>\n",
       "      <td>22.126009</td>\n",
       "      <td>22.240772</td>\n",
       "      <td>22.249970</td>\n",
       "      <td>21.786615</td>\n",
       "      <td>21.844309</td>\n",
       "      <td>21.621812</td>\n",
       "      <td>21.801295</td>\n",
       "      <td>21.549061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>2.395218</td>\n",
       "      <td>1.944178</td>\n",
       "      <td>2.198261</td>\n",
       "      <td>2.049267</td>\n",
       "      <td>1.874288</td>\n",
       "      <td>1.955498</td>\n",
       "      <td>2.174163</td>\n",
       "      <td>2.258517</td>\n",
       "      <td>1.976967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>17.195400</td>\n",
       "      <td>17.642933</td>\n",
       "      <td>17.577600</td>\n",
       "      <td>18.038200</td>\n",
       "      <td>17.616800</td>\n",
       "      <td>17.789933</td>\n",
       "      <td>10.487300</td>\n",
       "      <td>17.499200</td>\n",
       "      <td>17.548200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>20.581300</td>\n",
       "      <td>20.881425</td>\n",
       "      <td>20.767500</td>\n",
       "      <td>20.988000</td>\n",
       "      <td>20.547612</td>\n",
       "      <td>20.574563</td>\n",
       "      <td>20.090075</td>\n",
       "      <td>20.143362</td>\n",
       "      <td>20.174600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>22.041500</td>\n",
       "      <td>22.259142</td>\n",
       "      <td>22.213000</td>\n",
       "      <td>22.046400</td>\n",
       "      <td>21.821000</td>\n",
       "      <td>21.742600</td>\n",
       "      <td>21.715650</td>\n",
       "      <td>21.811200</td>\n",
       "      <td>21.613567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>23.869200</td>\n",
       "      <td>23.349800</td>\n",
       "      <td>23.709133</td>\n",
       "      <td>23.437387</td>\n",
       "      <td>23.166867</td>\n",
       "      <td>23.178300</td>\n",
       "      <td>22.908800</td>\n",
       "      <td>23.152983</td>\n",
       "      <td>22.813250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>28.654867</td>\n",
       "      <td>27.416800</td>\n",
       "      <td>28.243267</td>\n",
       "      <td>27.652000</td>\n",
       "      <td>26.534800</td>\n",
       "      <td>26.420467</td>\n",
       "      <td>26.453950</td>\n",
       "      <td>27.162000</td>\n",
       "      <td>25.819400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "SENSOR_ID           1            2            3            4            6   \\\n",
       "count      6754.000000  6754.000000  6754.000000  6754.000000  6754.000000   \n",
       "mean         22.192462    22.126009    22.240772    22.249970    21.786615   \n",
       "std           2.395218     1.944178     2.198261     2.049267     1.874288   \n",
       "min          17.195400    17.642933    17.577600    18.038200    17.616800   \n",
       "25%          20.581300    20.881425    20.767500    20.988000    20.547612   \n",
       "50%          22.041500    22.259142    22.213000    22.046400    21.821000   \n",
       "75%          23.869200    23.349800    23.709133    23.437387    23.166867   \n",
       "max          28.654867    27.416800    28.243267    27.652000    26.534800   \n",
       "\n",
       "SENSOR_ID           7            8            9            10  \n",
       "count      6754.000000  6754.000000  6754.000000  6754.000000  \n",
       "mean         21.844309    21.621812    21.801295    21.549061  \n",
       "std           1.955498     2.174163     2.258517     1.976967  \n",
       "min          17.789933    10.487300    17.499200    17.548200  \n",
       "25%          20.574563    20.090075    20.143362    20.174600  \n",
       "50%          21.742600    21.715650    21.811200    21.613567  \n",
       "75%          23.178300    22.908800    23.152983    22.813250  \n",
       "max          26.420467    26.453950    27.162000    25.819400  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_samp.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our data set smoothly tracks Temperature over a 2 minute interval without undefined data points. Let's plot our findings for each sensor in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x12d501e10>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x11beba210>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x11c68fd50>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x11e750f50>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x11e880a90>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x11e8bec90>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x11e8f97d0>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x11e92be50>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x11e9369d0>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_samp.plot(subplots=True, legend=True, figsize=(10,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have collected our time-series data as a vector $\\vec{T}$, where $\\vec{T}$ contains all of the time samples for each sensor value. This can be represented as:\n",
    "\n",
    "$\\vec{T}=\\lbrace\\langle s_1\\cdots s_n\\rangle|1\\leq k\\leq n, s_k\\in S\\rbrace$, where $S$ is the set of sensors transmitting to the gateway.\n",
    "\n",
    "We choose to use an autoencoder for several reasons:\n",
    "\n",
    "    1.Responds well to seasonal data\n",
    "    2.Sending the decoder to the cloud is a one-time operation\n",
    "    3.Trains quickly\n",
    "\n",
    "The goal of the autoencoder is to send a _representation_ of the input batch that is smaller in size (less bytes) but can be decoded to reveal the same information. Using our time-series data, we will generate a training set that mimics a batch over a windowed time period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AE](./img/ae.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our autoencoder input will be an arbitrarily chosen length, with $n$ dimensions for each input. We will iteratively train the encoder in an attempt to reconstruct the original vectors (as close as possible). Once we have a reasonable reconstruction, we can send the yellow nodes to the cloud. Incoming data will be encoded (seen in red) and sent to the cloud as a smaller representation. Let us begin by choosing a window size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will turn this time-series problem into a cross-sectional problem by taking a sliding window of vectors and applying the result to a standard autoencoder. First, let us drop values to make our window compatible with an autoencoder input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>SENSOR_ID</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATETIME</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2004-03-01 00:00:00</td>\n",
       "      <td>18.449800</td>\n",
       "      <td>18.864667</td>\n",
       "      <td>18.753600</td>\n",
       "      <td>19.11130</td>\n",
       "      <td>18.6752</td>\n",
       "      <td>18.707050</td>\n",
       "      <td>18.386100</td>\n",
       "      <td>18.484100</td>\n",
       "      <td>18.430200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2004-03-01 00:02:00</td>\n",
       "      <td>18.440000</td>\n",
       "      <td>18.848333</td>\n",
       "      <td>18.756867</td>\n",
       "      <td>19.10640</td>\n",
       "      <td>18.6654</td>\n",
       "      <td>18.692350</td>\n",
       "      <td>18.378750</td>\n",
       "      <td>18.469400</td>\n",
       "      <td>18.415500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2004-03-01 00:04:00</td>\n",
       "      <td>18.440000</td>\n",
       "      <td>18.832000</td>\n",
       "      <td>18.734000</td>\n",
       "      <td>19.10640</td>\n",
       "      <td>18.6654</td>\n",
       "      <td>18.685000</td>\n",
       "      <td>18.376300</td>\n",
       "      <td>18.475933</td>\n",
       "      <td>18.400800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2004-03-01 00:06:00</td>\n",
       "      <td>18.437550</td>\n",
       "      <td>18.851600</td>\n",
       "      <td>18.753600</td>\n",
       "      <td>19.10640</td>\n",
       "      <td>18.6654</td>\n",
       "      <td>18.677650</td>\n",
       "      <td>18.377933</td>\n",
       "      <td>18.482467</td>\n",
       "      <td>18.410600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2004-03-01 00:08:00</td>\n",
       "      <td>18.435100</td>\n",
       "      <td>18.861400</td>\n",
       "      <td>18.773200</td>\n",
       "      <td>19.10150</td>\n",
       "      <td>18.6556</td>\n",
       "      <td>18.685000</td>\n",
       "      <td>18.371400</td>\n",
       "      <td>18.479200</td>\n",
       "      <td>18.433467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2004-03-10 08:56:00</td>\n",
       "      <td>22.864700</td>\n",
       "      <td>23.119500</td>\n",
       "      <td>23.830000</td>\n",
       "      <td>23.96965</td>\n",
       "      <td>24.0946</td>\n",
       "      <td>24.071733</td>\n",
       "      <td>25.263250</td>\n",
       "      <td>25.971300</td>\n",
       "      <td>24.403300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2004-03-10 08:58:00</td>\n",
       "      <td>22.835300</td>\n",
       "      <td>23.178300</td>\n",
       "      <td>23.776100</td>\n",
       "      <td>23.93045</td>\n",
       "      <td>24.0946</td>\n",
       "      <td>23.991700</td>\n",
       "      <td>25.296733</td>\n",
       "      <td>26.044800</td>\n",
       "      <td>24.473533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2004-03-10 09:00:00</td>\n",
       "      <td>22.879400</td>\n",
       "      <td>23.134200</td>\n",
       "      <td>23.717300</td>\n",
       "      <td>23.92800</td>\n",
       "      <td>24.1044</td>\n",
       "      <td>23.928000</td>\n",
       "      <td>25.395550</td>\n",
       "      <td>26.113400</td>\n",
       "      <td>24.589500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2004-03-10 09:02:00</td>\n",
       "      <td>22.869600</td>\n",
       "      <td>23.121950</td>\n",
       "      <td>23.676467</td>\n",
       "      <td>23.90840</td>\n",
       "      <td>24.1485</td>\n",
       "      <td>23.957400</td>\n",
       "      <td>25.496000</td>\n",
       "      <td>26.280000</td>\n",
       "      <td>24.692400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2004-03-10 09:04:00</td>\n",
       "      <td>22.836933</td>\n",
       "      <td>23.161150</td>\n",
       "      <td>23.673200</td>\n",
       "      <td>23.89370</td>\n",
       "      <td>24.1436</td>\n",
       "      <td>23.947600</td>\n",
       "      <td>25.518050</td>\n",
       "      <td>26.270200</td>\n",
       "      <td>24.709550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6753 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "SENSOR_ID                   1          2          3         4        6   \\\n",
       "DATETIME                                                                  \n",
       "2004-03-01 00:00:00  18.449800  18.864667  18.753600  19.11130  18.6752   \n",
       "2004-03-01 00:02:00  18.440000  18.848333  18.756867  19.10640  18.6654   \n",
       "2004-03-01 00:04:00  18.440000  18.832000  18.734000  19.10640  18.6654   \n",
       "2004-03-01 00:06:00  18.437550  18.851600  18.753600  19.10640  18.6654   \n",
       "2004-03-01 00:08:00  18.435100  18.861400  18.773200  19.10150  18.6556   \n",
       "...                        ...        ...        ...       ...      ...   \n",
       "2004-03-10 08:56:00  22.864700  23.119500  23.830000  23.96965  24.0946   \n",
       "2004-03-10 08:58:00  22.835300  23.178300  23.776100  23.93045  24.0946   \n",
       "2004-03-10 09:00:00  22.879400  23.134200  23.717300  23.92800  24.1044   \n",
       "2004-03-10 09:02:00  22.869600  23.121950  23.676467  23.90840  24.1485   \n",
       "2004-03-10 09:04:00  22.836933  23.161150  23.673200  23.89370  24.1436   \n",
       "\n",
       "SENSOR_ID                   7          8          9          10  \n",
       "DATETIME                                                         \n",
       "2004-03-01 00:00:00  18.707050  18.386100  18.484100  18.430200  \n",
       "2004-03-01 00:02:00  18.692350  18.378750  18.469400  18.415500  \n",
       "2004-03-01 00:04:00  18.685000  18.376300  18.475933  18.400800  \n",
       "2004-03-01 00:06:00  18.677650  18.377933  18.482467  18.410600  \n",
       "2004-03-01 00:08:00  18.685000  18.371400  18.479200  18.433467  \n",
       "...                        ...        ...        ...        ...  \n",
       "2004-03-10 08:56:00  24.071733  25.263250  25.971300  24.403300  \n",
       "2004-03-10 08:58:00  23.991700  25.296733  26.044800  24.473533  \n",
       "2004-03-10 09:00:00  23.928000  25.395550  26.113400  24.589500  \n",
       "2004-03-10 09:02:00  23.957400  25.496000  26.280000  24.692400  \n",
       "2004-03-10 09:04:00  23.947600  25.518050  26.270200  24.709550  \n",
       "\n",
       "[6753 rows x 9 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "# Effectively dropping the last data_samp[0] mod 3 values.\n",
    "data_window = data_samp[:math.floor(data_samp.shape[0]/window)*window]\n",
    "data_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new data window size will guarantee the autoencoder input will be consistent. Now we will define a function to create a sliding window over each timestep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_window(arr, window):\n",
    "    iterator = iter(arr)\n",
    "    n_windows = len(arr)-window+1\n",
    "    for i in range(n_windows):\n",
    "        yield arr[i:i+window]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_window_slide = list(build_window(data_samp.values, window))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our data is in a sliding window of 3 vectors with 9 sensor dimensions. We would like to concatenate the vectors in each windowed batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_window_slide)):\n",
    "    data_window_slide[i] = data_window_slide[i].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([18.4498    , 18.86466667, 18.7536    , 19.1113    , 18.6752    ,\n",
       "        18.70705   , 18.3861    , 18.4841    , 18.4302    , 18.44      ,\n",
       "        18.84833333, 18.75686667, 19.1064    , 18.6654    , 18.69235   ,\n",
       "        18.37875   , 18.4694    , 18.4155    , 18.44      , 18.832     ,\n",
       "        18.734     , 19.1064    , 18.6654    , 18.685     , 18.3763    ,\n",
       "        18.47593333, 18.4008    ]),\n",
       " array([18.44      , 18.84833333, 18.75686667, 19.1064    , 18.6654    ,\n",
       "        18.69235   , 18.37875   , 18.4694    , 18.4155    , 18.44      ,\n",
       "        18.832     , 18.734     , 19.1064    , 18.6654    , 18.685     ,\n",
       "        18.3763    , 18.47593333, 18.4008    , 18.43755   , 18.8516    ,\n",
       "        18.7536    , 19.1064    , 18.6654    , 18.67765   , 18.37793333,\n",
       "        18.48246667, 18.4106    ]),\n",
       " array([18.44      , 18.832     , 18.734     , 19.1064    , 18.6654    ,\n",
       "        18.685     , 18.3763    , 18.47593333, 18.4008    , 18.43755   ,\n",
       "        18.8516    , 18.7536    , 19.1064    , 18.6654    , 18.67765   ,\n",
       "        18.37793333, 18.48246667, 18.4106    , 18.4351    , 18.8614    ,\n",
       "        18.7732    , 19.1015    , 18.6556    , 18.685     , 18.3714    ,\n",
       "        18.4792    , 18.43346667])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_window_slide[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of taking the time component out is that we are able to randomize our training representations. Each $n\\times k$ value (where $k$ is the batch window size) describes a feature of the graph above, but is still a learnable representation in any order. Due to the nature of the data, we do not need to account for seasonal data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to prevent gradients from vanishing, we need to normalize our data, so we will pack our data to values in the range $\\lbrack0,1\\rbrack$. To accomplish this we can use the following formula:\n",
    "\n",
    "$\\text{norm($x_i$)}=\\frac{x_i-\\text{min($x$)}}{\\text{max($x$)}-\\text{min($x$)}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data: list):\n",
    "    normed = data\n",
    "    min_x = min([min(x) for x in data])\n",
    "    max_x = max([max(x) for x in data])\n",
    "    denom = max_x - min_x\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            normed[i][j] = (data[i][j]-min_x) / denom\n",
    "    return normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_window_norm = normalize(data_window_slide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.43828104, 0.46111661, 0.45500315, 0.47469208, 0.45068776,\n",
       "        0.45244089, 0.43477479, 0.44016902, 0.43720219, 0.43774162,\n",
       "        0.46021757, 0.45518295, 0.47442237, 0.45014834, 0.45163175,\n",
       "        0.43437022, 0.43935988, 0.43639306, 0.43774162, 0.45931853,\n",
       "        0.4539243 , 0.47442237, 0.45014834, 0.45122719, 0.43423537,\n",
       "        0.4397195 , 0.43558393]),\n",
       " array([0.43774162, 0.46021757, 0.45518295, 0.47442237, 0.45014834,\n",
       "        0.45163175, 0.43437022, 0.43935988, 0.43639306, 0.43774162,\n",
       "        0.45931853, 0.4539243 , 0.47442237, 0.45014834, 0.45122719,\n",
       "        0.43423537, 0.4397195 , 0.43558393, 0.43760676, 0.46039737,\n",
       "        0.45500315, 0.47442237, 0.45014834, 0.45082262, 0.43432527,\n",
       "        0.44007912, 0.43612335]),\n",
       " array([0.43774162, 0.45931853, 0.4539243 , 0.47442237, 0.45014834,\n",
       "        0.45122719, 0.43423537, 0.4397195 , 0.43558393, 0.43760676,\n",
       "        0.46039737, 0.45500315, 0.47442237, 0.45014834, 0.45082262,\n",
       "        0.43432527, 0.44007912, 0.43612335, 0.43747191, 0.4609368 ,\n",
       "        0.45608199, 0.47415266, 0.44960892, 0.45122719, 0.43396566,\n",
       "        0.43989931, 0.437382  ])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_window_norm[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, test_size=0.1):\n",
    "    X_train, X_test = train_test_split(data, test_size=test_size)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = split_data(data_window_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 6076, Test size: 676\n"
     ]
    }
   ],
   "source": [
    "print('Train size: {}, Test size: {}'.format(len(train), len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "input_size = len(data_window_slide)\n",
    "n_features = len(data_window_slide[0])\n",
    "encoding_dim = window\n",
    "hidden_size = 100\n",
    "\n",
    "input_layer = Input(shape=(input_size, n_features))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "output_layer = Dense(input_size, activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = Model(input_layer, output_layer)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        (None, 6752, 27)          0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 6752, 3)           84        \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 6752, 6752)        27008     \n",
      "=================================================================\n",
      "Total params: 27,092\n",
      "Trainable params: 27,092\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt to fit the autoencoder. Note the target of the autoencoder is the training data as well, so the input is supplied as the target. The testing sets will be used for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 6076 arrays: [array([[0.56882136],\n       [0.58824058],\n       [0.58122809],\n       [0.59417423],\n       [0.56693338],\n       [0.57223771],\n       [0.54805358],\n       [0.54508676],\n       [0.54427762],\n       [0....",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-4c4d65b587c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 6076 arrays: [array([[0.56882136],\n       [0.58824058],\n       [0.58122809],\n       [0.59417423],\n       [0.56693338],\n       [0.57223771],\n       [0.54805358],\n       [0.54508676],\n       [0.54427762],\n       [0...."
     ]
    }
   ],
   "source": [
    "history = autoencoder.fit(train, train,\n",
    "    epochs=10,\n",
    "    batch_size=10,\n",
    "    validation_data=(test, test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Train vs Validation Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Encoder-Decoder (Future Work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an array of 9 sensor values with readings at every 2 minute interval, we would like to generate a compressed representation of these values, like the section above.\n",
    "\n",
    "Let us consider the same vector $\\vec{T}$ as our sensors temperature representation, but this time as time-series set.\n",
    "\n",
    "Using the scale from $x_1\\cdots x_{Tx}$ as a batch, we could choose a size similar to our sliding window in the previous concept.\n",
    "\n",
    "This approach is different from above in that it offers a key capability: *prediction*. Using the LSTM approach, we will be able to predict values, and potentially send predictions to the cloud before readings are made. If an anomaly is detected in our actual readings we can update the cloud data, but in the average case this will outperform a traditional gateway. Of course, there are trade-offs to this approach. The upside is there will be no need to send the decoder to the cloud, since we can send batches of predicted values instead. The downside is the performance, and time to train is high.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AUTOENCODER](./img/autoencoder.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
